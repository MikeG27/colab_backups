{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MikeG27/colab_backups/blob/main/Agentic_AI_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1VFgNPv2SUk"
      },
      "source": [
        "# LlamaIndex - Another step into Agentic World!\n",
        "\n",
        "Welcome to the world of **LlamaIndex**, where AI agents gain the ability to understand, index, and retrieve information efficiently! In this notebook, we will explore how LlamaIndex enhances **knowledge retrieval and orchestration** in agentic workflows.\n",
        "\n",
        "We will dive into the **core concepts**, set up a working environment, and experiment with LlamaIndex’s capabilities through hands-on examples.\n",
        "\n",
        "## Overview\n",
        "LlamaIndex is a powerful **data framework** designed to help AI agents seamlessly **connect, retrieve, and utilize** structured and unstructured knowledge. It acts as a **bridge** between AI models and external data sources, enabling efficient memory and reasoning in multi-agent workflows.\n",
        "\n",
        "## Why LlamaIndex?\n",
        "LlamaIndex provides key advantages in agentic workflows:\n",
        "\n",
        "- **Flexible Data Integration:** Supports a variety of data sources, including databases, documents, and APIs.\n",
        "- **Efficient Retrieval:** Utilizes **vector-based search** and **keyword indexing** to fetch relevant information quickly.\n",
        "- **Seamless AI Compatibility:** Integrates with **LLMs**, allowing agents to **understand, analyze, and generate** knowledge-based responses.\n",
        "- **Optimized for Agentic Systems:** Works smoothly with multi-agent frameworks like CrewAI, LangChain, and OpenAI tools.\n",
        "\n",
        "## Core Concepts in LlamaIndex\n",
        "LlamaIndex is built on a few fundamental abstractions:\n",
        "\n",
        "- **Index:** A structured representation of data that enables efficient retrieval (e.g., vector index, keyword index).\n",
        "- **Node:** The smallest unit of indexed data (e.g., a paragraph, document, or dataset entry).\n",
        "- **Retriever:** A mechanism that searches and returns the most relevant information based on user queries.\n",
        "- **Query Engine:** Uses indexing and retrieval techniques to answer questions from external data sources.\n",
        "- **Storage:** Manages persistence for indexed knowledge, ensuring efficient lookups.\n",
        "\n",
        "These components work together to **enhance AI agents' memory and reasoning capabilities**, making them smarter and more context-aware.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbnZCOuB14v4"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "Before diving into examples, we need to install and configure LlamaIndex. The following commands will install LlamaIndex and its dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjawk_QA15ud",
        "outputId": "16aa4ed0-9079-4dbf-ac9c-ce1183cb8a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.11/dist-packages (0.12.24)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-cli<0.5.0,>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.24 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.12.24)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.6.8)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.3)\n",
            "Requirement already satisfied: llama-index-program-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.1)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.3.0)\n",
            "Requirement already satisfied: llama-index-readers-file<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.6)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from llama-index) (0.4.0)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.24->llama-index) (1.17.2)\n",
            "Requirement already satisfied: llama-cloud<0.2.0,>=0.1.13 in /usr/local/lib/python3.11/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (0.1.14)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (5.3.1)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.4.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.14.0)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.4 in /usr/local/lib/python3.11/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (0.6.5)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.24->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.24->llama-index) (3.26.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.24->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: llama-index-llms-azure-openai in /usr/local/lib/python3.11/dist-packages (0.3.2)\n",
            "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai) (1.21.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai) (0.12.24)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai) (0.3.25)\n",
            "Requirement already satisfied: azure-core>=1.31.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.32.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.32.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2024.10.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.17.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-openai<0.4.0,>=0.3.0->llama-index-llms-azure-openai) (1.61.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-llms-azure-openai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-llms-azure-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-llms-azure-openai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-llms-azure-openai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.18.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.17.1)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2024.11.6)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.0->llama-index-llms-azure-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.0->llama-index-llms-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->llama-index-llms-openai<0.4.0,>=0.3.0->llama-index-llms-azure-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (3.26.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-llms-azure-openai) (24.2)\n",
            "Requirement already satisfied: llama-index-embeddings-azure-openai in /usr/local/lib/python3.11/dist-packages (0.3.0)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-azure-openai) (0.12.24)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-azure-openai) (0.3.1)\n",
            "Requirement already satisfied: llama-index-llms-azure-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-azure-openai) (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.11.13)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.10.6)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.17.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.61.1)\n",
            "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.21.0)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (0.3.25)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.18.3)\n",
            "Requirement already satisfied: azure-core>=1.31.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.32.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (43.0.3)\n",
            "Requirement already satisfied: msal>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.32.0)\n",
            "Requirement already satisfied: msal-extensions>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2024.11.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (3.26.1)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (1.17.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-embeddings-azure-openai) (24.2)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (2.10.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai<0.4.0,>=0.3.0->llama-index-embeddings-azure-openai) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-azure-openai\n",
        "!pip install llama-index-embeddings-azure-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkm5x0eB529K"
      },
      "source": [
        "## Setup API Keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaBmI81-5CE_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://saturn-poc1.openai.azure.com/\"\n",
        "os.environ[\"OPENAI_API_VERSION\"] = \"2024-05-01-preview\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlDXcIfn50HZ"
      },
      "source": [
        "## Setting Up the LLM\n",
        "The LLM is instantiated with a specific model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q7A6yIS5j4B"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "llm = AzureOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
        ")\n",
        "\n",
        "# You need to deploy your own embedding model as well as your own chat completion model\n",
        "embed_model = AzureOpenAIEmbedding(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    deployment_name=\"text-embedding-3-small\",\n",
        "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
        ")\n",
        "\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evXri_-J3Ta-"
      },
      "source": [
        "# Example 1: Single agent with Math Tools\n",
        "\n",
        "### Description\n",
        "This example demonstrates how to create a single agent that can perform basic mathematical operations (addition, subtraction, multiplication) using function tools. The agent processes a given query, performs calculations, and returns the final result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcVi2H313T19"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent import (\n",
        "    FunctionCallingAgentWorker,\n",
        "    ReActAgent,\n",
        ")\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM7r4QEhWbp6"
      },
      "source": [
        "### Tools definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REVqRQ1v3VAz"
      },
      "outputs": [],
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "subtract_tool = FunctionTool.from_defaults(fn=subtract)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtRU7n_PWdji"
      },
      "source": [
        "### Define an agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeD5jOXI3Whq"
      },
      "outputs": [],
      "source": [
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, subtract_tool], llm=llm, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbRpF0nQ6Chp",
        "outputId": "25f0088b-ab04-46bf-c970-88c41b99f899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Running step c72ed023-c354-4cec-a510-beeb97bf8d2a. Step input: What is (26 * 2) + 2024?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {'a': 26, 'b': 2}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 52\n",
            "\u001b[0m> Running step b34b7bd5-c658-46a3-9f6a-0c9b0c1a0b0d. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: Now I have the result of (26 * 2), which is 52. I will add this to 2024.\n",
            "Action: add\n",
            "Action Input: {'a': 52, 'b': 2024}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 2076\n",
            "\u001b[0m> Running step 90bdf331-9d99-4dbe-862c-a2e712bd52bd. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
            "Answer: The result of (26 * 2) + 2024 is 2076.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"What is (26 * 2) + 2024?\")\n",
        "#display(HTML(f'<p style=\"font-size:20px\">{response.response}</p>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "189OIdRZcFn2"
      },
      "source": [
        "### Summary\n",
        "This example showcases how to attach the tools to LlamaIndex to integrate simple mathematical operations into an AI agent workflow. The agent correctly interprets and executes the computation in a structured manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-hRMt-6Wvs8"
      },
      "source": [
        "# Example 2: Building an agent Reasoning Loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUb1KRVrcRzo"
      },
      "source": [
        "### Description\n",
        "This example demonstrates a more advanced agent with reasoning capabilities. The agent is designed to process questions related to the MetaGPT framework and retrieve relevant information from a given document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alneol6Mddv2"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, SummaryIndex\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
        "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
        "from typing import List, Optional\n",
        "\n",
        "def get_doc_tools(\n",
        "    file_path: str,\n",
        "    name: str,\n",
        ") -> str:\n",
        "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
        "\n",
        "    # Load documents\n",
        "    documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n",
        "    splitter = SentenceSplitter(chunk_size=1024)\n",
        "    nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Define Azure OpenAI embedding model\n",
        "    embed_model = AzureOpenAIEmbedding(\n",
        "      model=\"text-embedding-3-small\",\n",
        "      deployment_name=\"text-embedding-3-small\",\n",
        "      api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
        "      azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "      api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
        "    )\n",
        "\n",
        "    # Use Azure embedding model in VectorStoreIndex\n",
        "    vector_index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "\n",
        "    def vector_query(\n",
        "        query: str,\n",
        "        page_numbers: Optional[List[str]] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Use to answer questions over a given paper.\"\"\"\n",
        "\n",
        "        page_numbers = page_numbers or []\n",
        "        metadata_dicts = [\n",
        "            {\"key\": \"page_label\", \"value\": p} for p in page_numbers\n",
        "        ]\n",
        "\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=2,\n",
        "            filters=MetadataFilters.from_dicts(\n",
        "                metadata_dicts,\n",
        "                condition=FilterCondition.OR\n",
        "            )\n",
        "        )\n",
        "        response = query_engine.query(query)\n",
        "        return response\n",
        "\n",
        "    vector_query_tool = FunctionTool.from_defaults(\n",
        "        name=f\"vector_tool_{name}\",\n",
        "        fn=vector_query\n",
        "    )\n",
        "\n",
        "    summary_index = SummaryIndex(nodes)\n",
        "    # Pass the llm to the as_query_engine function\n",
        "    summary_query_engine = summary_index.as_query_engine(\n",
        "        llm=llm, # This line is added to specify the LLM\n",
        "        response_mode=\"tree_summarize\",\n",
        "        use_async=True,\n",
        "    )\n",
        "    summary_tool = QueryEngineTool.from_defaults(\n",
        "        name=f\"summary_tool_{name}\",\n",
        "        query_engine=summary_query_engine,\n",
        "        description=(\n",
        "            f\"Useful for summarization questions related to {name}\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    return vector_query_tool, summary_tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J1FNhdDW8a_"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "vector_tool, summary_tool = get_doc_tools(\"metagpt.pdf\", \"metagpt\")\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool, summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC4rucUfXETr",
        "outputId": "f372980f-a86f-4272-b0a0-b04528c3663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "In MetaGPT, agent roles are specialized positions that contribute to the software development process, each with distinct responsibilities. These roles include Product Manager, Architect, Engineer, and Project Manager, among others. The framework emphasizes the importance of these roles in enhancing collaboration and efficiency by following established standards and workflows. The presence of multiple roles leads to improved code generation, reduced human revision costs, and higher executability of the generated code. The effectiveness of these roles is demonstrated through experiments showing that their inclusion consistently enhances performance metrics, such as revisions and executability.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"communication between agents in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "In the MetaGPT framework, communication between agents is facilitated through shared development and communication protocols. This allows various agents to collaborate effectively, enabling them to work together on more complex tasks or projects. The framework supports the integration of multiple agents, each potentially possessing different emotions, personalities, and capabilities tailored for specific tasks. This structured approach enhances the overall coordination and interaction among agents, leading to more coherent and efficient problem-solving processes.\n",
            "=== LLM Response ===\n",
            "In MetaGPT, agent roles are specialized positions that contribute to the software development process, each with distinct responsibilities. Key roles include:\n",
            "\n",
            "- **Product Manager**: Oversees the product vision and strategy.\n",
            "- **Architect**: Designs the system architecture.\n",
            "- **Engineer**: Implements the code and technical solutions.\n",
            "- **Project Manager**: Manages timelines and resources.\n",
            "\n",
            "These roles enhance collaboration and efficiency by adhering to established standards and workflows. The presence of multiple roles leads to improved code generation, reduced human revision costs, and higher executability of the generated code. Experiments have shown that including these roles consistently enhances performance metrics, such as revisions and executability.\n",
            "\n",
            "Regarding communication, agents in the MetaGPT framework communicate through shared development and communication protocols. This structured approach allows various agents to collaborate effectively on complex tasks or projects. Each agent may possess different emotions, personalities, and capabilities tailored for specific tasks, which enhances overall coordination and interaction. This leads to more coherent and efficient problem-solving processes among the agents.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HywYJDpcXJoU",
        "outputId": "203cc284-452b-46fb-a46b-8929e87eaff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_label: 2\n",
            "file_name: metagpt.pdf\n",
            "file_path: metagpt.pdf\n",
            "file_type: application/pdf\n",
            "file_size: 16911937\n",
            "creation_date: 2025-03-14\n",
            "last_modified_date: 2025-03-14\n",
            "\n",
            "Preprint\n",
            "Figure 1: The software development SOPs between MetaGPT and real-world human teams.\n",
            "In software engineering, SOPs promote collaboration among various roles. MetaGPT showcases\n",
            "its ability to decompose complex tasks into specific actionable procedures assigned to various roles\n",
            "(e.g., Product Manager, Architect, Engineer, etc.).\n",
            "documents, design artifacts, flowcharts, and interface specifications. The use of intermediate struc-\n",
            "tured outputs significantly increases the success rate of target code generation. Because it helps\n",
            "maintain consistency in communication, minimizing ambiguities and errors during collaboration.\n",
            "More graphically, in a company simulated by MetaGPT, all employees follow a strict and stream-\n",
            "lined workflow, and all their handovers must comply with certain established standards. This reduces\n",
            "the risk of hallucinations caused by idle chatter between LLMs, particularly in role-playing frame-\n",
            "works, like: “ Hi, hello and how are you?” – Alice (Product Manager); “ Great! Have you had\n",
            "lunch?” – Bob (Architect).\n",
            "Benefiting from SOPs, MetaGPT offers a promising approach to meta-programming. In this context,\n",
            "we adopt meta-programming1 as ”programming to program”, in contrast to the broader fields of meta\n",
            "learning and ”learning to learn” (Schmidhuber, 1987; 1993a; Hochreiter et al., 2001; Schmidhuber,\n",
            "2006; Finn et al., 2017).\n",
            "This notion of meta-programming also encompasses earlier efforts like CodeBERT (Feng et al.,\n",
            "2020) and recent projects such as CodeLlama (Rozi `ere et al., 2023) and WizardCoder (Luo\n",
            "et al., 2023). However, MetaGPT stands out as a unique solution that allows for efficient meta-\n",
            "programming through a well-organized group of specialized agents. Each agent has a specific role\n",
            "and expertise, following some established standards. This allows for automatic requirement analysis,\n",
            "system design, code generation, modification, execution, and debugging during runtime, highlight-\n",
            "ing how agent-based techniques can enhance meta-programming.\n",
            "To validate the design of MetaGPT, we use publicly available HumanEval (Chen et al., 2021a) and\n",
            "MBPP (Austin et al., 2021) for evaluations. Notably, in code generation benchmarks, MetaGPT\n",
            "achieves a new state-of-the-art (SoTA) with 85.9% and 87.7% in Pass@1. When compared to other\n",
            "popular frameworks for creating complex software projects, such as AutoGPT (Torantulino et al.,\n",
            "2023), LangChain (Chase, 2022), AgentVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\n",
            "MetaGPT also stands out in handling higher levels of software complexity and offering extensive\n",
            "functionality. Remarkably, in our experimental evaluations, MetaGPT achieves a 100% task com-\n",
            "pletion rate, demonstrating the robustness and efficiency (time and token costs) of our design.\n",
            "We summarize our contributions as follows:\n",
            "1https://en.wikipedia.org/w/index.php?title=Metaprogramming\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk6zAmiYXOgs",
        "outputId": "c10f8b6b-589b-4213-e2c6-750788e5bb9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me the results over one of the above datasets.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"results over one of the datasets in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "MetaGPT demonstrates superior performance on the MBPP and HumanEval benchmarks, achieving pass rates of 85.9% and 87.7%, respectively. This performance surpasses all previous approaches, indicating its effectiveness in generating executable code. Additionally, when paired with GPT-4, MetaGPT significantly enhances the Pass @k metrics in the HumanEval benchmark.\n",
            "=== LLM Response ===\n",
            "MetaGPT shows impressive results on the MBPP and HumanEval benchmarks, achieving pass rates of 85.9% and 87.7%, respectively. These results surpass all previous approaches, highlighting MetaGPT's effectiveness in generating executable code. Furthermore, when combined with GPT-4, MetaGPT significantly improves the Pass @k metrics in the HumanEval benchmark, demonstrating enhanced performance in code generation tasks.\n"
          ]
        }
      ],
      "source": [
        "response = agent.chat(\"Tell me the results over one of the above datasets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAHDydTGXSww"
      },
      "source": [
        "## Lower-Level: Debuggability and control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O670h97XXa_8"
      },
      "outputs": [],
      "source": [
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [vector_tool, summary_tool],\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts5JxBnXXgDl"
      },
      "outputs": [],
      "source": [
        "task = agent.create_task(\n",
        "    \"Tell me about the agent roles in MetaGPT, \"\n",
        "    \"and then how they communicate with each other.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWGIr3auXjdY",
        "outputId": "03174def-aae1-412d-cc9c-a92aa6b1db01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the agent roles in MetaGPT, and then how they communicate with each other.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"agent roles in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "In MetaGPT, agent roles are specialized positions that contribute to the software development process. Each role, such as Product Manager, Architect, and Engineer, has specific expertise and responsibilities, allowing for a structured workflow. This role-based task management enhances collaboration and efficiency, as agents follow established standards and procedures. The inclusion of multiple roles has been shown to improve code quality, reduce human revision costs, and increase the overall executability of the generated code. The effectiveness of these roles is demonstrated through experiments, where the addition of different roles consistently leads to better performance outcomes.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"how agents communicate in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "Agents in MetaGPT communicate using a shared message pool to publish structured messages. They can subscribe to relevant messages based on their profiles, facilitating efficient information exchange. This structured communication is enhanced by a communication protocol that improves role communication efficiency. Additionally, agents can obtain directional information from other roles and public information from the environment, which aids in their collaborative efforts.\n"
          ]
        }
      ],
      "source": [
        "step_output = agent.run_step(task.task_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q37zTkcrXnQp",
        "outputId": "bb508062-fef6-4c89-f400-6bf65281e8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num completed for task 51137ae9-2bc1-4af7-ae34-92789c6aedf7: 1\n",
            "In MetaGPT, agent roles are specialized positions that contribute to the software development process. Each role, such as Product Manager, Architect, and Engineer, has specific expertise and responsibilities, allowing for a structured workflow. This role-based task management enhances collaboration and efficiency, as agents follow established standards and procedures. The inclusion of multiple roles has been shown to improve code quality, reduce human revision costs, and increase the overall executability of the generated code. The effectiveness of these roles is demonstrated through experiments, where the addition of different roles consistently leads to better performance outcomes.\n"
          ]
        }
      ],
      "source": [
        "completed_steps = agent.get_completed_steps(task.task_id)\n",
        "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
        "print(completed_steps[0].output.sources[0].raw_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dasiDKIHXr_N",
        "outputId": "78f97fdf-d626-4dd9-b277-0cb0811e2a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num upcoming steps for task 51137ae9-2bc1-4af7-ae34-92789c6aedf7: 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TaskStep(task_id='51137ae9-2bc1-4af7-ae34-92789c6aedf7', step_id='40a2a96c-6997-4aa0-9d27-4bf8e2e9622f', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
        "print(f\"Num upcoming steps for task {task.task_id}: {len(upcoming_steps)}\")\n",
        "upcoming_steps[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIgI_NwHXtxD",
        "outputId": "47614111-6269-4028-b1a9-24aaf4ba643e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: What about how agents share information?\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_metagpt with args: {\"query\": \"how agents share information in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "Agents within the framework collaborate by adhering to defined development and communication protocols. This allows them to share information effectively while working together on complex tasks or projects.\n"
          ]
        }
      ],
      "source": [
        "step_output = agent.run_step(\n",
        "    task.task_id, input=\"What about how agents share information?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1A9h8bXXvg2",
        "outputId": "d8e309b2-3794-48b7-cc6a-6324035dbd50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== LLM Response ===\n",
            "In MetaGPT, agents share information by adhering to defined development and communication protocols. This structured approach facilitates effective collaboration, enabling agents to work together on complex tasks or projects efficiently. They utilize a shared message pool to publish and subscribe to relevant messages, ensuring that information is exchanged in an organized manner.\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "step_output = agent.run_step(task.task_id)\n",
        "print(step_output.is_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbbcORkFXw-u",
        "outputId": "d294e39f-beb7-4684-a910-83586b895d86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In MetaGPT, agents share information by adhering to defined development and communication protocols. This structured approach facilitates effective collaboration, enabling agents to work together on complex tasks or projects efficiently. They utilize a shared message pool to publish and subscribe to relevant messages, ensuring that information is exchanged in an organized manner.\n"
          ]
        }
      ],
      "source": [
        "response = agent.finalize_response(task.task_id)\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTNBsjT8cZr9"
      },
      "source": [
        "## Summary\n",
        "This example illustrates how an AI agent can extract and summarize information from documents efficiently using LlamaIndex's vector and summary tools. It also highlights structured communication within an agent-based workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNT8FqmxDZ7l"
      },
      "source": [
        "# Example 3: Multi doc agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd0hECJ5cipk"
      },
      "source": [
        "### Description\n",
        "This example expands on the previous ones by introducing an agent capable of handling multiple documents. The agent downloads, indexes, and queries multiple research papers for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24xF5mSRDiWW"
      },
      "outputs": [],
      "source": [
        "def download_papers(urls, papers):\n",
        "    \"\"\"Downloads the papers from the given URLs if they are not already present.\"\"\"\n",
        "    for url, paper in zip(urls, papers):\n",
        "        if not Path(paper).exists():\n",
        "            print(f\"Downloading {paper}...\")\n",
        "            response = requests.get(url)\n",
        "            with open(paper, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            print(f\"Downloaded {paper} successfully.\")\n",
        "        else:\n",
        "            print(f\"{paper} already exists. Skipping download.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePFmv9t9DkLd",
        "outputId": "d1c9f59b-52ec-475e-eda5-4895c63144ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metagpt.pdf already exists. Skipping download.\n",
            "longlora.pdf already exists. Skipping download.\n",
            "selfrag.pdf already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]\n",
        "\n",
        "download_papers(urls, papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMCLqlmqQYPO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDH6QvbnEl2_",
        "outputId": "20b59617-8166-4883-b3eb-4f98a019dec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<llama_index.core.tools.function_tool.FunctionTool at 0x783c85cd51d0>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x783c87a17550>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x783c85abff90>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x783c85a96210>,\n",
              " <llama_index.core.tools.function_tool.FunctionTool at 0x783c85b3b890>,\n",
              " <llama_index.core.tools.query_engine.QueryEngineTool at 0x783c85b3fed0>]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "\n",
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
        "initial_tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzfLMfHeE5ss"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-iU9srEE94O",
        "outputId": "7ecc4289-b5ba-4ee2-c8ac-571020a9361b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset is not explicitly mentioned in the provided context. However, it discusses the performance of the proposed framework on cross-dataset and cross-manipulation evaluations, indicating that various datasets may have been used for evaluation purposes.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results indicate that the proposed model, fine-tuned on a context length of 16,384, demonstrates comparable or superior performance to other long-context models, including GPT-3.5-Turbo and various Llama2-based models. Specifically, in the LongBench benchmark, the model achieved an average score of 36.8 across different tasks, while in the LEval benchmark, it recorded a win rate of 39.06, outperforming several competitors. The fine-tuning process was efficient, taking approximately 4 hours and utilizing around 0.3 billion tokens on a single 8× A100 machine.\n",
            "=== LLM Response ===\n",
            "### Evaluation Dataset in LongLoRA\n",
            "The evaluation dataset used in LongLoRA is not explicitly detailed in the provided context. However, it mentions that the framework's performance was assessed through cross-dataset and cross-manipulation evaluations, suggesting that multiple datasets may have been utilized for these evaluations.\n",
            "\n",
            "### Evaluation Results\n",
            "The evaluation results indicate that the LongLoRA model, which was fine-tuned with a context length of 16,384, shows comparable or superior performance to other long-context models, including GPT-3.5-Turbo and various Llama2-based models. Specifically, in the LongBench benchmark, the model achieved an average score of 36.8 across different tasks. In the LEval benchmark, it recorded a win rate of 39.06, outperforming several competitors. The fine-tuning process was efficient, taking approximately 4 hours and utilizing around 0.3 billion tokens on a single 8× A100 machine.\n",
            "### Evaluation Dataset in LongLoRA\n",
            "The evaluation dataset used in LongLoRA is not explicitly detailed in the provided context. However, it mentions that the framework's performance was assessed through cross-dataset and cross-manipulation evaluations, suggesting that multiple datasets may have been utilized for these evaluations.\n",
            "\n",
            "### Evaluation Results\n",
            "The evaluation results indicate that the LongLoRA model, which was fine-tuned with a context length of 16,384, shows comparable or superior performance to other long-context models, including GPT-3.5-Turbo and various Llama2-based models. Specifically, in the LongBench benchmark, the model achieved an average score of 36.8 across different tasks. In the LEval benchmark, it recorded a win rate of 39.06, outperforming several competitors. The fine-tuning process was efficient, taking approximately 4 hours and utilizing around 0.3 billion tokens on a single 8× A100 machine.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7uzseIzSFe4",
        "outputId": "cff4b6b6-6fd5-4977-a3a0-723fae17847b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG (Self Retrieval-Augmented Generation) is a framework that enhances the capabilities of language models by integrating retrieval mechanisms directly into the generation process. It allows models to access external knowledge dynamically during inference, improving their ability to generate accurate and contextually relevant responses. Self-RAG operates by retrieving relevant documents or information from a knowledge base based on the input query and then using this information to inform the generation of responses. This approach helps in reducing hallucinations and improving the factual accuracy of the generated content.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA (Long-Range Low-Rank Adaptation) is a method designed to improve the efficiency and effectiveness of fine-tuning large language models. It focuses on adapting models to handle long-context inputs by utilizing low-rank adaptations, which reduce the computational burden while maintaining performance. LongLoRA enables models to process longer sequences of text without a significant increase in resource requirements, making it suitable for applications that involve extensive context, such as document summarization and long-form content generation.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG (Self Retrieval-Augmented Generation) is a framework that enhances the capabilities of language models by integrating retrieval mechanisms directly into the generation process. It allows models to access external knowledge dynamically during inference, improving their ability to generate accurate and contextually relevant responses. Self-RAG operates by retrieving relevant documents or information from a knowledge base based on the input query and then using this information to inform the generation of responses. This approach helps in reducing hallucinations and improving the factual accuracy of the generated content.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA (Long-Range Low-Rank Adaptation) is a method designed to improve the efficiency and effectiveness of fine-tuning large language models. It focuses on adapting models to handle long-context inputs by utilizing low-rank adaptations, which reduce the computational burden while maintaining performance. LongLoRA enables models to process longer sequences of text without a significant increase in resource requirements, making it suitable for applications that involve extensive context, such as document summarization and long-form content generation.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG (Self Retrieval-Augmented Generation) is a framework that enhances the capabilities of language models by integrating retrieval mechanisms directly into the generation process. It allows models to access external knowledge dynamically during inference, improving their ability to generate accurate and contextually relevant responses. Self-RAG operates by retrieving relevant documents or information from a knowledge base based on the input query and then using this information to inform the generation of responses. This approach helps in reducing hallucinations and improving the factual accuracy of the generated content.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA (Long-Range Low-Rank Adaptation) is a method designed to improve the efficiency and effectiveness of fine-tuning large language models. It focuses on adapting models to handle long-context inputs by utilizing low-rank adaptations, which reduce the computational burden while maintaining performance. LongLoRA enables models to process longer sequences of text without a significant increase in resource requirements, making it suitable for applications that involve extensive context, such as document summarization and long-form content generation.\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== LLM Response ===\n",
            "It seems that I'm currently unable to access the summarization tools due to a rate limit issue. However, I can provide a brief summary based on my knowledge:\n",
            "\n",
            "### Self-RAG (Self Retrieval-Augmented Generation)\n",
            "Self-RAG is a framework that enhances language models by integrating retrieval mechanisms into the generation process. It allows models to dynamically access external knowledge during inference, improving the accuracy and relevance of generated responses. By retrieving relevant documents based on the input query, Self-RAG helps reduce hallucinations and enhances factual accuracy.\n",
            "\n",
            "### LongLoRA (Long-Range Low-Rank Adaptation)\n",
            "LongLoRA is a method aimed at improving the fine-tuning of large language models, particularly for handling long-context inputs. It employs low-rank adaptations to reduce computational demands while maintaining performance. This approach enables models to process longer sequences of text efficiently, making it suitable for tasks like document summarization and long-form content generation.\n",
            "\n",
            "If you need more detailed information or specific aspects of these frameworks, please let me know!\n",
            "It seems that I'm currently unable to access the summarization tools due to a rate limit issue. However, I can provide a brief summary based on my knowledge:\n",
            "\n",
            "### Self-RAG (Self Retrieval-Augmented Generation)\n",
            "Self-RAG is a framework that enhances language models by integrating retrieval mechanisms into the generation process. It allows models to dynamically access external knowledge during inference, improving the accuracy and relevance of generated responses. By retrieving relevant documents based on the input query, Self-RAG helps reduce hallucinations and enhances factual accuracy.\n",
            "\n",
            "### LongLoRA (Long-Range Low-Rank Adaptation)\n",
            "LongLoRA is a method aimed at improving the fine-tuning of large language models, particularly for handling long-context inputs. It employs low-rank adaptations to reduce computational demands while maintaining performance. This approach enables models to process longer sequences of text efficiently, making it suitable for tasks like document summarization and long-form content generation.\n",
            "\n",
            "If you need more detailed information or specific aspects of these frameworks, please let me know!\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o9GHGO1hFCYK"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwIwnhXSFH-R",
        "outputId": "7d02b304-b7fa-451a-b09f-b783741e81d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metagpt.pdf already exists. Skipping download.\n",
            "longlora.pdf already exists. Skipping download.\n",
            "loftq.pdf already exists. Skipping download.\n",
            "swebench.pdf already exists. Skipping download.\n",
            "selfrag.pdf already exists. Skipping download.\n",
            "zipformer.pdf already exists. Skipping download.\n",
            "values.pdf already exists. Skipping download.\n",
            "finetune_fair_diffusion.pdf already exists. Skipping download.\n",
            "knowledge_card.pdf already exists. Skipping download.\n",
            "metra.pdf already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "download_papers(urls, papers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzK3LktyFMmd",
        "outputId": "fd6560d3-ad05-42b5-f116-697656a66182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n",
            "Getting tools for paper: metra.pdf\n"
          ]
        }
      ],
      "source": [
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
        "\n",
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DKKpgP2tFOTy"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y9NDMC1sFP-U"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a7yk7U8OFTYI"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fUmHYQe5FW4-",
        "outputId": "1a00bc35-82c8-43b9-acd4-fec8702362f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "Encountered error: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2024-05-01-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 60 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_swebench with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset consists of task instances that meet specific criteria during the construction and validation processes. A task instance is included if it has at least one test that changes from fail to pass. The dataset is finalized after removing instances that do not meet these criteria. The finalized task instances are saved in a single .json file, which is open-sourced and available for download. Additionally, a corresponding folder is created containing the ground truth test results for each task instance, organized by their test status transitions.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_swebench with args: {\"query\": \"MetaGPT evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset for MetaGPT is not explicitly mentioned in the provided information. However, there is a reference to an evaluation test set that includes a breakdown of model performance across various repositories, as well as statistics related to task instances and their associated tags. If you are looking for specific details about the MetaGPT evaluation dataset, that information is not available in the context provided.\n",
            "=== LLM Response ===\n",
            "### Evaluation Dataset Comparison: MetaGPT vs. SWE-Bench\n",
            "\n",
            "#### SWE-Bench Evaluation Dataset\n",
            "- **Construction Criteria**: The evaluation dataset consists of task instances that meet specific criteria during the construction and validation processes. A task instance is included if it has at least one test that changes from fail to pass.\n",
            "- **Finalization**: The dataset is finalized after removing instances that do not meet the criteria.\n",
            "- **Format**: The finalized task instances are saved in a single .json file, which is open-sourced and available for download.\n",
            "- **Ground Truth**: A corresponding folder is created containing the ground truth test results for each task instance, organized by their test status transitions.\n",
            "\n",
            "#### MetaGPT Evaluation Dataset\n",
            "- **Details**: The evaluation dataset for MetaGPT is not explicitly detailed in the provided information. However, it includes an evaluation test set that breaks down model performance across various repositories.\n",
            "- **Statistics**: There are statistics related to task instances and their associated tags, but specific details about the dataset's construction or criteria are not available.\n",
            "\n",
            "### Summary\n",
            "- **SWE-Bench** has a clearly defined evaluation dataset with specific criteria for inclusion, a structured format, and ground truth results.\n",
            "- **MetaGPT** lacks detailed information about its evaluation dataset, focusing instead on model performance metrics and task instance statistics without a clear description of the dataset's construction or criteria. \n",
            "\n",
            "This indicates that SWE-Bench provides a more transparent and structured approach to its evaluation dataset compared to MetaGPT.\n",
            "### Evaluation Dataset Comparison: MetaGPT vs. SWE-Bench\n",
            "\n",
            "#### SWE-Bench Evaluation Dataset\n",
            "- **Construction Criteria**: The evaluation dataset consists of task instances that meet specific criteria during the construction and validation processes. A task instance is included if it has at least one test that changes from fail to pass.\n",
            "- **Finalization**: The dataset is finalized after removing instances that do not meet the criteria.\n",
            "- **Format**: The finalized task instances are saved in a single .json file, which is open-sourced and available for download.\n",
            "- **Ground Truth**: A corresponding folder is created containing the ground truth test results for each task instance, organized by their test status transitions.\n",
            "\n",
            "#### MetaGPT Evaluation Dataset\n",
            "- **Details**: The evaluation dataset for MetaGPT is not explicitly detailed in the provided information. However, it includes an evaluation test set that breaks down model performance across various repositories.\n",
            "- **Statistics**: There are statistics related to task instances and their associated tags, but specific details about the dataset's construction or criteria are not available.\n",
            "\n",
            "### Summary\n",
            "- **SWE-Bench** has a clearly defined evaluation dataset with specific criteria for inclusion, a structured format, and ground truth results.\n",
            "- **MetaGPT** lacks detailed information about its evaluation dataset, focusing instead on model performance metrics and task instance statistics without a clear description of the dataset's construction or criteria. \n",
            "\n",
            "This indicates that SWE-Bench provides a more transparent and structured approach to its evaluation dataset compared to MetaGPT.\n"
          ]
        }
      ],
      "source": [
        "agent = AgentRunner(agent_worker)\n",
        "\n",
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bZM2g1sjUMfG",
        "outputId": "00a325b2-5e63-43fb-8fe0-0cf754b2c58f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"Analyze the approach of the LongLoRA paper.\"}\n",
            "=== Function Output ===\n",
            "The LongLoRA paper introduces an efficient fine-tuning method aimed at extending the context sizes of pre-trained large language models (LLMs) while minimizing computational costs. The approach leverages two main strategies: the use of sparse local attention during training and the integration of improved parameter-efficient fine-tuning techniques.\n",
            "\n",
            "One of the key innovations is the shifted sparse attention (S2-Attn), which allows for effective context extension with significant computational savings compared to traditional dense global attention. This method can be implemented with minimal code changes during training and remains optional during inference, ensuring that the original architecture is preserved.\n",
            "\n",
            "Additionally, LongLoRA enhances the LoRA (Low-Rank Adaptation) framework by incorporating trainable normalization and embedding, which bridges the gap between LoRA and full fine-tuning. This combination enables the extension of context lengths for models like Llama2, achieving impressive results on various tasks while maintaining compatibility with existing techniques.\n",
            "\n",
            "Overall, LongLoRA demonstrates a strong balance between efficiency and performance, allowing for substantial increases in context length without the extensive resource requirements typically associated with training LLMs.\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_loftq with args: {\"query\": \"Analyze the approach of the LoftQ paper.\"}\n",
            "=== Function Output ===\n",
            "The LoftQ approach focuses on quantizing pre-trained models while optimizing low-rank adapters to enhance performance in natural language understanding (NLU) and natural language generation (NLG) tasks. It employs an alternating optimization algorithm that iteratively refines quantized weights and low-rank approximations, allowing for efficient fine-tuning without updating the entire model's parameters.\n",
            "\n",
            "LoftQ integrates various quantization methods, including uniform quantization and NF4/NF2, demonstrating compatibility with different quantization functions. The method achieves significant compression ratios, enabling reduced model size while maintaining performance. Mixed-precision quantization is also explored, yielding notable accuracy improvements on specific datasets.\n",
            "\n",
            "The experiments conducted with LoftQ reveal its effectiveness compared to baseline methods, such as full fine-tuning and QLoRA. The results indicate that LoftQ can enhance downstream task performance with fewer training costs, particularly through its alternating optimization strategy, which shows substantial improvements even with minimal steps. Overall, LoftQ presents a promising solution for efficient model adaptation in resource-constrained environments.\n",
            "=== LLM Response ===\n",
            "### Comparison of Approaches in LongLoRA and LoftQ Papers\n",
            "\n",
            "#### LongLoRA\n",
            "- **Objective**: LongLoRA aims to extend the context sizes of pre-trained large language models (LLMs) while minimizing computational costs.\n",
            "- **Key Strategies**:\n",
            "  - **Sparse Local Attention**: Utilizes a method called shifted sparse attention (S2-Attn) to allow for effective context extension with significant computational savings compared to traditional dense global attention.\n",
            "  - **Parameter-Efficient Fine-Tuning**: Enhances the LoRA (Low-Rank Adaptation) framework by incorporating trainable normalization and embedding, bridging the gap between LoRA and full fine-tuning.\n",
            "- **Implementation**: The approach can be implemented with minimal code changes during training and remains optional during inference, preserving the original architecture.\n",
            "- **Performance**: Achieves impressive results on various tasks while maintaining compatibility with existing techniques, demonstrating a strong balance between efficiency and performance.\n",
            "\n",
            "#### LoftQ\n",
            "- **Objective**: LoftQ focuses on quantizing pre-trained models while optimizing low-rank adapters to enhance performance in natural language understanding (NLU) and natural language generation (NLG) tasks.\n",
            "- **Key Strategies**:\n",
            "  - **Alternating Optimization Algorithm**: Iteratively refines quantized weights and low-rank approximations, allowing for efficient fine-tuning without updating the entire model's parameters.\n",
            "  - **Quantization Methods**: Integrates various quantization methods, including uniform quantization and NF4/NF2, achieving significant compression ratios and reduced model size while maintaining performance.\n",
            "  - **Mixed-Precision Quantization**: Explores mixed-precision quantization, yielding notable accuracy improvements on specific datasets.\n",
            "- **Performance**: Demonstrates effectiveness compared to baseline methods, such as full fine-tuning and QLoRA, enhancing downstream task performance with fewer training costs, particularly through its alternating optimization strategy.\n",
            "\n",
            "### Summary of Differences\n",
            "- **Focus**: LongLoRA emphasizes extending context lengths in LLMs, while LoftQ concentrates on model quantization and efficient fine-tuning.\n",
            "- **Techniques**: LongLoRA employs sparse attention and enhancements to the LoRA framework, whereas LoftQ utilizes quantization and an alternating optimization algorithm.\n",
            "- **Implementation and Efficiency**: LongLoRA maintains the original architecture with minimal changes, while LoftQ achieves significant model compression and performance improvements through quantization techniques.\n",
            "\n",
            "Overall, both papers present innovative approaches to improving the efficiency and performance of large language models, but they target different aspects of model adaptation and optimization.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2Hj7x3-efiP"
      },
      "source": [
        "### Summary\n",
        "This example demonstrates a scalable approach to document-based knowledge retrieval and summarization, allowing an agent to process and analyze multiple documents efficiently.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rhHhhRde-Lh"
      },
      "source": [
        "# Call to Action!\n",
        "\n",
        "Now that you've seen how LlamaIndex can be used for knowledge retrieval and agentic workflows, it's time to try it yourself!\n",
        "\n",
        "1. **Choose a Document** – Pick a document (e.g., a research paper, report, or dataset) that you’d like to analyze.\n",
        "2. **Index and Process** – Use the `SimpleDirectoryReader`, `VectorStoreIndex`, and `SummaryIndex` to structure your data.\n",
        "3. **Ask Questions** – Create an agent to query your document and extract meaningful insights.\n",
        "\n",
        "Experiment with different configurations and retrieval methods to enhance your AI’s understanding of the content. Happy coding! 🚀\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTl5LeIdfCL0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOX0RMsddkGZZubYEJ5jWan",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}